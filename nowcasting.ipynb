{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.signal import lfilter\n",
    "\n",
    "from scipy.linalg import eigh, block_diag\n",
    "from numpy.linalg import inv, det, pinv\n",
    "from numpy import kron, log, eye, diag, zeros, ones, empty\n",
    "\n",
    "idx = pd.IndexSlice\n",
    "\n",
    "from pandas.tseries.offsets import MonthBegin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InitCond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### InitCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def InitCond(X, r, p, spec, blocks, optNaN, Rcon, q):\n",
    "    \n",
    "    nQ = sum(spec.Frequency.eq('q'));   # Number of quarterly series\n",
    "    Qv = spec.loc[spec.Frequency.eq('q')].index.tolist()\n",
    "    Mv = spec.loc[spec.Frequency.eq('m')].index.tolist()\n",
    "\n",
    "    pC = Rcon.shape[1]    # 5, 'tent' structure size (quarterly to monthly)\n",
    "    ppC = max(p, pC)\n",
    "\n",
    "    # OPTS = pd.Series({'disp':0}) # turns off diagnostic information for eigenvalue computation\n",
    "    xBal, indNaN = remNaNs_spline(X, optNaN)\n",
    "\n",
    "    T, N = xBal.shape\n",
    "\n",
    "    xNaN = xBal.copy()\n",
    "    xNaN[indNaN] = np.nan   # set missing values equal to NaN\n",
    "\n",
    "    res = xBal.copy()\n",
    "    resNaN = xNaN.copy()\n",
    "\n",
    "    # Initialize model coefficient output\n",
    "    # C = np.empty((25, 0), int)\n",
    "    C = pd.DataFrame()\n",
    "    A = pd.DataFrame()\n",
    "    Q = pd.DataFrame()\n",
    "    V_0 = pd.DataFrame()\n",
    "\n",
    "    # Set the first observations as NaNs: For quarterly-monthly aggreg. scheme\n",
    "    indNaN.iloc[:pC-1, :] = True\n",
    "\n",
    "    for r_i, b in zip(r, blocks.columns):    # loop for each block\n",
    "\n",
    "        # Observation eq.\n",
    "        C_i = pd.DataFrame(np.zeros((N, r_i*ppC)), index=spec.index)\n",
    "        # lags = [0, 1, ..., ppC], factors = [1, 2, ..., r_i]\n",
    "        C_i.columns = pd.MultiIndex.from_product([range(ppC), range(1, r_i+1)]) \n",
    "        idx_iM = blocks.loc[spec.Frequency.eq('m') & blocks[b].eq(1)].index\n",
    "        idx_iQ = blocks.loc[spec.Frequency.eq('q') & blocks[b].eq(1)].index\n",
    "\n",
    "        # Return eigenvectors v with largest r_i eigenvalues d\n",
    "        d, v = eigh(res.loc[:, idx_iM].cov(), subset_by_index=[idx_iM.shape[0]-r_i, idx_iM.shape[0]-1])\n",
    "        v = pd.DataFrame(v, index=idx_iM, columns=list(range(1, r_i+1)))\n",
    "\n",
    "        # Flip sign for cleaner output. Gives equivalent results without this section\n",
    "        v *= ((sum(v.squeeze()) > 0) * 2 - 1)\n",
    "\n",
    "        # For monthly series with loaded blocks (rows), replace with eigenvector\n",
    "        # This gives the loading\n",
    "        C_i.loc[idx_iM, 0] = pd.concat([v], axis=1, keys=[0])\n",
    "        f = res.loc[:, idx_iM].dot(v);  # Data projection for eigenvector direction\n",
    "        F = pd.DataFrame(index=X.iloc[ppC-1:].index)\n",
    "\n",
    "        # Lag matrix using loading. This is later used for quarterly series\n",
    "        for kk in range(max(p+1, pC)):\n",
    "            f_lag = pd.concat([f.shift(kk)], axis=1, keys=[kk])\n",
    "            F = pd.concat([F, f_lag], axis=1, join='inner')\n",
    "\n",
    "        Rcon_i = np.kron(Rcon, np.eye(r_i))  # Quarterly-monthly aggregate scheme\n",
    "        q_i = np.kron(q, np.zeros(r_i))      # Rcon_i * C_i = q_i\n",
    "\n",
    "        # Produces projected data with lag structure (so pC-1 fewer entries)\n",
    "        ff = F.iloc[:, :r_i*pC]\n",
    "        \n",
    "\n",
    "        for j in idx_iQ: # Loop for quarterly variables\n",
    "            # For series j, values are dropped to accommodate lag structure\n",
    "            xx_j = resNaN[j].iloc[pC-1:].copy()\n",
    "\n",
    "            if (~xx_j.isnull()).sum().squeeze() < ff.shape[1] + 2:\n",
    "                xx_j = res[j].iloc[pC-1:]              # Replaces xx_j with spline if too many NaNs\n",
    "\n",
    "            ff_j = ff.loc[~xx_j.isnull(), :]\n",
    "\n",
    "            xx_j = xx_j.loc[~xx_j.isnull()]\n",
    "            iff_j = inv(ff_j.T.dot(ff_j))\n",
    "\n",
    "            Cc = iff_j.dot(ff_j.T).dot(xx_j)    # least squares\n",
    "\n",
    "            # Spline data monthly to quarterly conversion\n",
    "            Cc = Cc - iff_j.dot(Rcon_i.T).dot(inv(Rcon_i.dot(iff_j).dot(Rcon_i.T))).dot(Rcon_i.dot(Cc))\n",
    "            # Cc = Cc - iff_j.dot(Rcon_i.T).dot(inv(Rcon_i.dot(iff_j).dot(Rcon_i.T))).dot((Rcon_i.dot(Cc)-q_i))\n",
    "\n",
    "            C_i.loc[j].iloc[:pC*r_i] = Cc\n",
    "\n",
    "            \n",
    "        # Zeros in first pC-1 entries (replace dropped from lag)\n",
    "        ff = pd.concat([pd.DataFrame(0, index=res.iloc[:pC-1].index, columns=ff.columns), ff], axis=0)\n",
    "\n",
    "        # Residual calculations\n",
    "        res = res.values - ff.dot(C_i.T)\n",
    "        resNaN = res.copy()\n",
    "        resNaN[indNaN] = np.nan\n",
    "\n",
    "        C = pd.concat([C, pd.concat([C_i], axis=1, keys=[b])], axis=1)\n",
    "\n",
    "        F.columns = pd.MultiIndex.from_tuples(F.columns)\n",
    "        F.columns.names = ['p', 'f']\n",
    "\n",
    "        ## Transition equation\n",
    "        z = F.loc[:, 0]    # Projected data (no lag)\n",
    "        Z = F.loc[:, 1:p]  # Data with lag 1\n",
    "\n",
    "        # Initialize transition matrix\n",
    "        # f_t = A * f_(t-1)\n",
    "        A_i = pd.DataFrame(0, index=F.columns, columns=[(i+1,j) for i,j in F.columns])\n",
    "        A_i.columns = pd.MultiIndex.from_tuples(A_i.columns)\n",
    "        A_temp = inv(Z.T.dot(Z)).dot(Z.T).dot(z)   # OLS coefficient of AR(p)\n",
    "\n",
    "        A_i.loc[idx[0, :], idx[:p, :]] = A_temp.T\n",
    "        A_i.loc[idx[1:, :], idx[:(ppC-1), :]] = np.eye(r_i*(ppC-1))\n",
    "\n",
    "        Q_i = pd.DataFrame(0, index=F.columns, columns=F.columns)\n",
    "        e = z.squeeze() - Z.dot(A_temp).squeeze()                  # VAR residuals\n",
    "        Q_i.loc[0, 0] = np.cov(e, rowvar=False)                    # VAR covariance matrix\n",
    "\n",
    "        initV_i = inv(eye((r_i*ppC)**2)-kron(A_i,A_i)).dot(Q_i.values.flatten('F')).reshape(r_i*ppC,r_i*ppC,order='F')\n",
    "        initV_i = pd.DataFrame(initV_i, index=Q_i.index, columns=Q_i.columns)\n",
    "\n",
    "        # Gives top left block for the transition matrix\n",
    "        A = bdkg_index(A, A_i, b)\n",
    "        Q = bdkg_index(Q, Q_i, b)\n",
    "        V_0 = bdkg_index(V_0, initV_i, b)\n",
    "        \n",
    "\n",
    "    for df in [A, Q, V_0]:\n",
    "        df.index = pd.MultiIndex.from_tuples(df.index)\n",
    "        df.columns = pd.MultiIndex.from_tuples(df.columns)\n",
    "\n",
    "\n",
    "    # C = [C Cm Cy]\n",
    "    Cm = pd.DataFrame(np.eye(N), index=spec.index, columns=spec.index).loc[:, Mv]\n",
    "    Cm.columns = pd.MultiIndex.from_product([Cm.columns, [0], [1]])\n",
    "    C = pd.concat([C, Cm], axis=1)\n",
    "\n",
    "    Cy = pd.DataFrame(0, index=spec.index, columns=pd.MultiIndex.from_product([Qv, range(pC), [1]]))\n",
    "    for j in Qv:\n",
    "        Cy.loc[j, j] = [1, 2, 3, 2, 1]\n",
    "\n",
    "    # Monthly-quarterly aggregate scheme\n",
    "    C = pd.concat([C, Cy], axis=1)\n",
    "    R = pd.DataFrame(np.diag(np.var(resNaN)), index=resNaN.columns, columns=resNaN.columns)\n",
    "\n",
    "    BM = pd.DataFrame(0, index=Mv, columns=Mv);        # Initialize monthly transition matrix values\n",
    "    SM = pd.DataFrame(0, index=Mv, columns=Mv);        # Initialize monthly residual covariance matrix values\n",
    "\n",
    "    for v in Mv:\n",
    "        # Set observation equation residual covariance matrix diagonal\n",
    "        R.loc[v, v] = 1e-04;\n",
    "\n",
    "        # Subsetting series residuals for series i\n",
    "        res_i = resNaN.loc[:, v].copy();\n",
    "\n",
    "        # Returns number of leading/ending zeros\n",
    "        rem_i = res_i.isnull()\n",
    "        leadZero = rem_i.cumsum() == list(range(1, rem_i.shape[0]+1))\n",
    "        endZero = (rem_i.sum() - rem_i.cumsum() + rem_i) == list(range(rem_i.shape[0], 0, -1))\n",
    "\n",
    "        # Truncate leading and ending zeros\n",
    "        res_i = res.loc[:, [v]].copy();\n",
    "        res_i = res_i.loc[~leadZero & ~endZero]\n",
    "\n",
    "        # Linear regression: AR 1 process for monthly series residuals\n",
    "        BM.loc[[v], [v]] = inv(res_i.iloc[:-1].T.dot(res_i.iloc[:-1])).dot(res_i.iloc[:-1].T).dot(res_i.iloc[1:])\n",
    "        SM.loc[[v], [v]] = (res_i-res_i.shift(1).dot(BM.loc[[v],[v]])).cov();  # Residual covariance matrix\n",
    "\n",
    "    sig_e = pd.DataFrame(0, index=Qv, columns=['sig']) \n",
    "\n",
    "    for v in Qv:\n",
    "        sig_e.loc[v] = R.loc[v, v] / 19.0\n",
    "        R.loc[v, v] = 1e-04                 # Covariance for obs matrix residuals\n",
    "\n",
    "    # For BQ, SQ\n",
    "    rho0 = 0.1;\n",
    "    temp = np.zeros((5,5));\n",
    "    temp[0,0] = 1;\n",
    "\n",
    "    # Blocks for covariance matrices\n",
    "    SQ = kron(np.diag((1-rho0**2)*sig_e.squeeze()), temp)\n",
    "    BQ = kron(np.eye(nQ), np.append([[rho0, 0, 0, 0, 0]], \n",
    "                                    np.append(np.eye(4), np.zeros((4,1)), axis=1), axis=0))\n",
    "    initViQ = inv(np.eye((ppC*nQ)**2)-kron(BQ,BQ)).dot(SQ.flatten('F')).reshape(ppC*nQ,ppC*nQ)\n",
    "    initViM = np.diag(1/np.diag(np.eye(BM.shape[0])-BM**2))*SM\n",
    "\n",
    "    BQ = pd.DataFrame(BQ, index=pd.MultiIndex.from_product([Qv, range(pC)]), \n",
    "                      columns=pd.MultiIndex.from_product([Qv, range(1, pC+1)]))\n",
    "\n",
    "    SQ = pd.DataFrame(SQ, index=pd.MultiIndex.from_product([Qv, range(pC)]), \n",
    "                      columns=pd.MultiIndex.from_product([Qv, range(pC)]))\n",
    "\n",
    "    initViQ = pd.DataFrame(initViQ, index=pd.MultiIndex.from_product([Qv, range(pC)]), \n",
    "                           columns=pd.MultiIndex.from_product([Qv, range(pC)]))\n",
    "\n",
    "    # Output\n",
    "    BM.index = BM.columns = Cm.columns\n",
    "    BQ.index = BQ.columns = Cy.columns\n",
    "    SM.index = SM.columns = Cm.columns\n",
    "    SQ.index = SQ.columns = Cy.columns\n",
    "    initViM.index = initViM.columns = Cm.columns\n",
    "    initViQ.index = initViQ.columns = Cy.columns\n",
    "    \n",
    "    A = bkdg_flat_index(bkdg_flat_index(A, BM), BQ)                # Observation matrix\n",
    "    Q = bkdg_flat_index(bkdg_flat_index(Q, SM), SQ)                # Residual covariance matrix (transition)\n",
    "    Z_0 = pd.Series(np.zeros(A.shape[0]), index=A.index)           # States\n",
    "    V_0 = bkdg_flat_index(bkdg_flat_index(V_0, initViM), initViQ)  # Covariance of states\n",
    "    \n",
    "    A.index = A.columns = pd.MultiIndex.from_tuples(A.index)\n",
    "    Q.index = Q.columns = pd.MultiIndex.from_tuples(Q.index)\n",
    "    V_0.index = V_0.columns = pd.MultiIndex.from_tuples(Q.index)\n",
    "    Z_0.index = pd.MultiIndex.from_tuples(Q.index)\n",
    "\n",
    "    return A, C, Q, R, Z_0, V_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_spec, load_data, nanLE, bdkg_index, bdkg_flat_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "code_folding": [
     0,
     28,
     70,
     79,
     85
    ]
   },
   "outputs": [],
   "source": [
    "def load_spec(filename):\n",
    "    spec = pd.read_excel(filename, sheet_name='spec');\n",
    "\n",
    "    # keep variables with Model == 1 and drop 'Model' column\n",
    "    spec = spec.loc[spec.Model.eq(1)].drop('Model', axis=1)\n",
    "\n",
    "    spec_sort = pd.DataFrame()\n",
    "    for f in ['d', 'w', 'm', 'q', 'sa', 'a']:\n",
    "        spec_sort = pd.concat([spec_sort, spec.loc[spec.Frequency.eq(f)]], axis = 0)\n",
    "\n",
    "    UnitsTransformed_dict = {'lin':'Levels (No Transformation)', 'chg':'Change (Difference)',\n",
    "                             'ch1':'Year over Year Change (Difference)', 'pch':'Percent Change',\n",
    "                             'pc1':'Year over Year Percent Change', 'pca':'Percent Change (Annual Rate)',\n",
    "                             'cch':'Continuously Compounded Rate of Change', \n",
    "                             'cca':'Continuously Compounded Annual Rate of Change', 'log':'Natural Log'}\n",
    "\n",
    "    spec_sort['UnitsTransformed'] = spec_sort['Transformation']\n",
    "    spec_sort['UnitsTransformed'] = spec_sort['UnitsTransformed'].replace(UnitsTransformed_dict)\n",
    "    spec_sort = spec_sort.set_index('SeriesID')\n",
    "    Blocks = spec_sort.filter(regex='^Block', axis=1)\n",
    "    Blocks.columns = [i for j,i in Blocks.columns.str.split('-').tolist()]\n",
    "    \n",
    "    spec_sort = spec_sort[spec_sort.columns.drop(list(spec_sort.filter(regex='Block')))]\n",
    "    spec_sort = pd.concat([spec_sort], axis=1, keys=['Spec'])\n",
    "    Blocks = pd.concat([Blocks], axis=1, keys=['Blocks'])\n",
    "\n",
    "    return pd.concat([spec_sort, Blocks], axis=1)\n",
    "\n",
    "def load_data(datafile, Spec, sample_start):\n",
    "    Z = pd.read_excel(datafile, sheet_name='data')\n",
    "    Z = Z.set_index('Date')\n",
    "    Z = Z.asfreq('MS')\n",
    "\n",
    "    if Z.dropna(how='all', axis=0).shape[0] != Z.shape[0]:\n",
    "        print('Data file has missing dates.')\n",
    "\n",
    "        Z.index.names = ['Time']\n",
    "    Z = Z.loc[:, Spec.index]\n",
    "\n",
    "    T, N = Z.shape\n",
    "\n",
    "    f2m = dict(zip(['m', 'q', 'sa', 'a'], [1, 3, 6, 12])) # \n",
    "    f2a = dict(zip(['m', 'q', 'sa', 'a'], [12, 4, 2, 1])) # \n",
    "\n",
    "    X = pd.DataFrame(np.nan, index=Z.index, columns=Z.columns)\n",
    "\n",
    "    for var in Z.columns:\n",
    "        formular = Spec.Spec.loc[var, 'Transformation']\n",
    "        freq = Spec.Spec.loc[var, 'Frequency']\n",
    "        if formular == 'lin':   # Levels (No Transformation)\n",
    "            X.loc[:, var] = Z.loc[:, var]\n",
    "        elif formular == 'chg': # Change (Difference)\n",
    "            X.loc[:, var] = Z.loc[:, var].diff(f2m[freq])\n",
    "        elif formular == 'ch1': # YoY Change (Difference)\n",
    "            X.loc[:, var] = Z.loc[:, var].diff(12)\n",
    "        elif formular == 'pch': # Percent Change\n",
    "            X.loc[:, var] = Z.loc[:, var].pct_change(f2m[freq], fill_method=None) * 100\n",
    "        elif formular == 'pc1': # YoY Percent Change\n",
    "            X.loc[:, var] = Z.loc[:, var].pct_change(12, fill_method=None) * 100\n",
    "        elif formular == 'pca': # Percent Change (annualised)\n",
    "            X.loc[:, var] = ((Z.loc[:, var].pct_change(f2m[freq], fill_method=None) + 1)**f2a[freq] - 1) * 100\n",
    "        elif formular == 'log': # Natural log\n",
    "            X.loc[:, var] = np.log(Z.loc[:, var])\n",
    "        else:\n",
    "            print('Transformation formular not found.')        \n",
    "\n",
    "    Z = Z.loc[sample_start:]\n",
    "    X = X.loc[sample_start:]\n",
    "    return X, Z\n",
    "\n",
    "def nanLE(rem1, X):\n",
    "    nanLead = rem1.cumsum() == list(range(1, rem1.shape[0] + 1))  # 위부터 연속하여 NaN인 행 식별\n",
    "    nanEnd = ((rem1.sum() - rem1.cumsum() + rem1)                 # rem1.cumsum(reverse=True)\n",
    "               == list(range(rem1.shape[0], 0, -1)))              # 아래부터 연속하여 NaN인 행 식별\n",
    "    nanLE = nanLead | nanEnd                                      # 위 또는 아래부터 연속하여 NaN인 행 식별\n",
    "    X = X.loc[~nanLE]\n",
    "    indNaN = X.isnull()\n",
    "    return X, indNaN\n",
    "\n",
    "def bdkg_index(A, A_i, b):\n",
    "    index = [(bb, ii, jj) for bb, ii, jj in A.index] + [(b, ii, jj) for ii, jj in A_i.index]\n",
    "    columns = [(bb, ii, jj) for bb, ii, jj in A.columns] + [(b, ii, jj) for ii, jj in A_i.columns]\n",
    "    A = pd.DataFrame(block_diag(A, A_i), index=index, columns=columns)\n",
    "    return A\n",
    "\n",
    "def bkdg_flat_index(A, B):\n",
    "    index = A.index.to_flat_index().tolist() + B.index.to_flat_index().tolist()\n",
    "    columns = A.columns.to_flat_index().tolist() + B.columns.to_flat_index().tolist()\n",
    "    A = pd.DataFrame(block_diag(A, B), index=index, columns=columns)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remNans_spline, filtering, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": [
     0,
     18
    ]
   },
   "outputs": [],
   "source": [
    "def filtering(X, k):\n",
    "    Y = X.copy()\n",
    "    for var in X.columns:\n",
    "        x = X[var].copy()\n",
    "        isnanx = x.isnull()\n",
    "        t1 = isnanx[isnanx.eq(False)].index[0]\n",
    "        t2 = isnanx[isnanx.eq(False)].index[-1]\n",
    "        x[t1:t2] = x[t1:t2].interpolate('cubic')  # GDPC1 with method 2: 1.4759/1.4731, 1.4277/1.4151 (matlab/python)\n",
    "        isnanx = x.isnull()\n",
    "        x.loc[x.isnull()] = x.median(skipna=True)\n",
    "        x_ext = np.append(np.append([x.values[0]]*k, x.values), [x.values[-1]]*k)\n",
    "        x_ma = lfilter(np.ones(2*k+1)/(2*k+1), 1, x_ext)  # equivalent to filter() in MATLAB\n",
    "        x_ma = x_ma[2*k:]\n",
    "        x_repl = pd.Series(x_ma, index=x.index)\n",
    "        x[isnanx] = x_repl[isnanx]                # very similar to matlab; 1.7855/1.7851, 1.8790/1.8795 (matlab/python)\n",
    "        Y.loc[:, var] = x\n",
    "    return Y\n",
    "\n",
    "def remNaNs_spline(X, optNaN): # spline without NaNs\n",
    "    indNaN = X.isnull()\n",
    "    T, N = X.shape\n",
    "\n",
    "    if optNaN.method==1:     # replace all the missing values\n",
    "        for var in X.columns:\n",
    "            x = X[var].copy()\n",
    "            isnanx = x.isnull()\n",
    "            x.loc[isnanx] = x.median(skipna=True)\n",
    "            x_ext = np.append(np.append([x.values[0]]*optNaN.k, x.values), [x.values[-1]]*optNaN.k)\n",
    "            x_ma = lfilter(np.ones(2*optNaN.k+1)/(2*optNaN.k+1), 1, x_ext)\n",
    "            x_ma = x_ma[2*k:]\n",
    "            x_repl = pd.Series(x_ma, index=x.index)\n",
    "            x[isnanx] = x_repl[isnanx]\n",
    "            X[var] = x\n",
    "        \n",
    "    elif optNaN.method==2:   # replace missing values after removing leading and closing zeros\n",
    "        rem1 = indNaN.sum(axis=1) > .8 * N                            # 80% 이상 변수의 값이 NaN인 행 식별(NaN행)\n",
    "        X, indNaN = nanLE(rem1, X)\n",
    "        X = filtering(X, optNaN.k)\n",
    "            \n",
    "    elif optNaN.method==3: # only remove rows with leading and closing zeros\n",
    "        rem1 = indNaN.sum(axis=1) == N                                # 모든 변수의 값이 NaN인 행 식별(NaN행)\n",
    "        X, indNaN = nanLE(rem1, X)\n",
    "        \n",
    "    elif optNaN.method==4:  # remove rows with leading and closing zeros & replace missing values\n",
    "        rem1 = indNaN.sum(axis=1) == N                                # 모든 변수의 값이 NaN인 행 식별(NaN행)\n",
    "        X, indNaN = nanLE(rem1, X)\n",
    "        X = filtering(X, optNaN.k)\n",
    "        \n",
    "    elif optNaN.method==5:  # replace missing values  \n",
    "        indNaN = X.isnull()\n",
    "        X = filtering(X, optNaN.k)\n",
    "    \n",
    "    return X, indNaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman filter/smoother"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MissData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def MissData(y, C, R):\n",
    "    # Syntax:\n",
    "    # Description:\n",
    "    #   Eliminates the rows in y & matrices C, R that correspond to missing \n",
    "    #   data (NaN) in y\n",
    "    #\n",
    "    # Input:\n",
    "    #   y: Vector of observations at time t\n",
    "    #   C: Observation matrix\n",
    "    #   R: Covariance for observation matrix residuals\n",
    "    #\n",
    "    # Output:\n",
    "    #   y: Vector of observations at time t (reduced)     \n",
    "    #   C: Observation matrix (reduced)     \n",
    "    #   R: Covariance for observation matrix residuals\n",
    "    #   L: Used to restore standard dimensions(n x #) where # is the nr of \n",
    "    #      available data in y\n",
    "\n",
    "    # Returns 1 for nonmissing series\n",
    "    ix = ~np.isnan(y)\n",
    "\n",
    "    # Index for columns with nonmissing variables\n",
    "    e = np.eye(y.shape[0])\n",
    "    L = e[:, ix]\n",
    "\n",
    "    # Removes missing series\n",
    "    y = y[ix]\n",
    "\n",
    "    # Removes missing series from observation matrix\n",
    "    C = C[ix, :]\n",
    "\n",
    "    # Removes missing series from transition matrix\n",
    "    R = R[ix, ix]\n",
    "\n",
    "    return y, C, R, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def FIS(A, Zm, ZmU, Vm, VmU, loglik, k_t):\n",
    "    # Applies fixed-interval smoother\n",
    "    #\n",
    "    #  Description:\n",
    "    #    SKF() applies a fixed-interval smoother, and is used in conjunction \n",
    "    #    with SKF(). See  page 154 of 'Forecasting, structural time series models \n",
    "    #    and the Kalman filter' for more details (Harvey, 1990).\n",
    "    #\n",
    "    #  Input parameters:\n",
    "    #    A: m-by-m transition matrix \n",
    "    #    S: structure returned by SKF() \n",
    "            #    S.Zm: m-by-nobs matrix, prior/predicted factor state vector\n",
    "            #          (S.Zm(:,t) = Z_t|t-1)\n",
    "            #    S.ZmU: m-by-(nobs+1) matrix, posterior/updated state vector\n",
    "            #           (S.Zm(t+1) = Z_t|t)\n",
    "            #    S.Vm: m-by-m-by-nobs array, prior/predicted covariance of factor\n",
    "            #          state vector (S.Vm(:,:,t) = V_t|t-1)  \n",
    "            #    S.VmU: m-by-m-by-(nobs+1) array, posterior/updated covariance of\n",
    "            #           factor state vector (S.VmU(:,:,t+1) = V_t|t)\n",
    "            #    S.loglik: scalar, value of likelihood function\n",
    "            #    S.k_t: k-by-m Kalman gain\n",
    "    #\n",
    "    #  Output parameters:\n",
    "    #    S: FIS() adds the following smoothed estimates to the S structure: \n",
    "    #    - S.ZmT: m-by-(nobs+1) matrix, smoothed states\n",
    "    #             (S.ZmT(:,t+1) = Z_t|T) \n",
    "    #    - S.VmT: m-by-m-by-(nobs+1) array, smoothed factor covariance\n",
    "    #             matrices (S.VmT(:,:,t+1) = V_t|T = Cov(Z_t|T))\n",
    "    #    - S.VmT_1: m-by-m-by-nobs array, smoothed lag 1 factor covariance\n",
    "    #               matrices (S.VmT_1(:,:,t) = Cov(Z_t Z_t-1|T))\n",
    "    #\n",
    "    #  Model:\n",
    "    #   Y_t = C_t Z_t + e_t for e_t ~ N(0, R)\n",
    "    #   Z_t = A Z_{t-1} + mu_t for mu_t ~ N(0, Q)\n",
    "\n",
    "    ## ORGANIZE INPUT ---------------------------------------------------------\n",
    "    # Initialize output matrices    \n",
    "    nobs, m = Zm.shape\n",
    "    ZmT = np.zeros((nobs+1,m))\n",
    "    VmT = np.zeros((nobs+1,m,m))\n",
    "\n",
    "    # Fill the final period of ZmT, VmT with SKF() posterior values\n",
    "    ZmT[nobs,:] = ZmU[nobs, :].squeeze()\n",
    "    VmT[nobs,:,:] = VmU[nobs,:,:].squeeze()\n",
    "\n",
    "    # Initialize VmT_1 lag 1 covariance matrix for final period\n",
    "    VmT_1 = np.zeros((nobs,m,m))\n",
    "    VmT_1[nobs-1,:,:] = (np.eye(m)-k_t).dot(A).dot(VmU[nobs-1,:,:].squeeze())\n",
    "\n",
    "    # Used for recursion process. See companion file for details\n",
    "    J_2 = VmU[nobs-1,:,:].squeeze().dot(A.T).dot(pinv(Vm[nobs-1,:,:]))\n",
    "\n",
    "    \n",
    "    ## RUN SMOOTHING ALGORITHM ----------------------------------------------\n",
    "    # Loop through time reverse-chronologically (starting at final period nobs)\n",
    "    for t in range(nobs-1, -1, -1):\n",
    "        # Store posterior and prior factor covariance values \n",
    "        VmUt = VmU[t,:,:].squeeze()\n",
    "        Vmt = Vm[t,:,:].squeeze()\n",
    "        \n",
    "        # Store previous period smoothed factor covariance and lag-1 covariance\n",
    "        VmT_t_1 = VmT[t+1,:,:].squeeze()\n",
    "        VmT_1t = VmT_1[t,:,:].squeeze()\n",
    "        \n",
    "        J_1 = J_2\n",
    "        \n",
    "        # Update smoothed factor estimate\n",
    "        ZmT[t,:] = ZmU[t,:] + J_1.dot(ZmT[t+1,:] - A.dot(ZmU[t,:]))\n",
    "        \n",
    "        # Update smoothed factor covariance matrix\n",
    "        VmT[t,:,:] = VmUt + J_1.dot(VmT_t_1 - Vmt).dot(J_1.T)\n",
    "        \n",
    "        \n",
    "        if t > 0:\n",
    "            # Update weight\n",
    "            J_2 = VmU[t-1,:,:].squeeze().dot(A.T).dot(pinv(Vm[t-1,:,:].squeeze()))\n",
    "            \n",
    "            # Update lag 1 factor covariance matrix \n",
    "            VmT_1[t-1,:,:] = VmUt.dot(J_2.T) + J_1.dot(VmT_1t - A.dot(VmUt)).dot(J_2.T)\n",
    "            \n",
    "\n",
    "    return ZmT, VmT, VmT_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def SKF(Y, A, C, Q, R, Z_0, V_0):\n",
    "    #  Description:\n",
    "    #    SKF() applies the Kalman filter\n",
    "    #\n",
    "    #  Input parameters:\n",
    "    #    Y: k-by-nobs matrix of input data\n",
    "    #    A: m-by-m transition matrix \n",
    "    #    C: k-by-m observation matrix\n",
    "    #    Q: m-by-m covariance matrix for transition equation residuals (mu_t)\n",
    "    #    R: k-by-k covariance for observation matrix residuals (e_t)\n",
    "    #    Z_0: 1-by-m vector, initial value of state\n",
    "    #    V_0: m-by-m matrix, initial value of state covariance matrix\n",
    "    #\n",
    "    #  Output parameters:\n",
    "    #    S.Zm: m-by-nobs matrix, prior/predicted factor state vector\n",
    "    #          (S.Zm(:,t) = Z_t|t-1)\n",
    "    #    S.ZmU: m-by-(nobs+1) matrix, posterior/updated state vector\n",
    "    #           (S.Zm(t+1) = Z_t|t)\n",
    "    #    S.Vm: m-by-m-by-nobs array, prior/predicted covariance of factor\n",
    "    #          state vector (S.Vm(:,:,t) = V_t|t-1)  \n",
    "    #    S.VmU: m-by-m-by-(nobs+1) array, posterior/updated covariance of\n",
    "    #           factor state vector (S.VmU(:,:,t+1) = V_t|t)\n",
    "    #    S.loglik: scalar, value of likelihood function\n",
    "    #    S.k_t: k-by-m Kalman gain\n",
    "\n",
    "    ## INITIALIZE OUTPUT VALUES ---------------------------------------------   \n",
    "    # Output structure & dimensions of state space matrix\n",
    "    k, m = C.shape   # k * m: observable variables and factors\n",
    "\n",
    "    # Outputs time for data matrix. \"number of observations\"\n",
    "    nobs  = Y.shape[1]\n",
    "\n",
    "    # Instantiate output\n",
    "    Zm     = np.ones((nobs, m)) * np.nan       # Z_t | t-1 (prior)\n",
    "    Vm     = np.ones((nobs, m, m)) * np.nan    # V_t | t-1 (prior)\n",
    "    ZmU    = np.ones((nobs+1, m)) * np.nan;    # Z_t | t (posterior/updated)\n",
    "    VmU    = np.ones((nobs+1, m, m)) * np.nan; # V_t | t (posterior/updated)\n",
    "    loglik = 0\n",
    "\n",
    "    ## SET INITIAL VALUES ----------------------------------------------------\n",
    "    Zu = Z_0.copy()  # Z_0|0 (In below loop, Zu gives Z_t | t)\n",
    "    Vu = V_0.copy()  # V_0|0 (In below loop, Vu guvse V_t | t)\n",
    "\n",
    "    # Store initial values\n",
    "    ZmU[0, :]     = Zu.copy()\n",
    "    VmU[0, :, :]  = Vu.copy()\n",
    "\n",
    "    ## KALMAN FILTER PROCEDURE ----------------------------------------------\n",
    "    for t in range(nobs):\n",
    "        \n",
    "        ### CALCULATING PRIOR DISTIBUTION----------------------------------\n",
    "        # Use transition eqn to create prior estimate for factor\n",
    "        # i.e. Z = Z_t|t-1\n",
    "        Z = A.dot(Zu)\n",
    "\n",
    "        # Prior covariance matrix of Z (i.e. V = V_t|t-1)\n",
    "        # Var(Z) = Var(A*Z+u_t) = A*Vu*A' + Q\n",
    "        V = A.dot(Vu).dot(A.T) + Q\n",
    "        V = (V+V.T)/2               # Trick to make symmetric\n",
    "\n",
    "        ### CALCULATING POSTERIOR DISTRIBUTION ----------------------------\n",
    "        # Removes missing series: These are removed from Y, C, and R\n",
    "        Y_t, C_t, R_t, L = MissData(Y[:, t], C, R)\n",
    "\n",
    "\n",
    "        # Check if y_t contains no data. If so, replace Zu and Vu with prior.\n",
    "        if Y_t.size==0:\n",
    "            Zu = Z\n",
    "            Vu = V\n",
    "        else:\n",
    "            # Steps for variance and population regression coefficients:\n",
    "            # Var(c_t*Z_t + e_t) = c_t Var(A) c_t' + Var(u) = c_t*V *c_t' + R\n",
    "            VC = V.dot(C_t.T)\n",
    "            iF = inv(C_t.dot(VC) + R_t)\n",
    "\n",
    "            # Matrix of population regression coefficients (QuantEcon eqn #4)\n",
    "            VCF = VC.dot(iF)\n",
    "\n",
    "            # Gives difference between actual and predicted observation\n",
    "            # matrix values\n",
    "            innov = Y_t - C_t.dot(Z)\n",
    "\n",
    "            # Update estimate of factor values (posterior)\n",
    "            Zu = Z  + VCF.dot(innov)\n",
    "\n",
    "            # Update covariance matrix (posterior) for time t\n",
    "            Vu = V  - VCF.dot(VC.T)\n",
    "            Vu = (Vu+Vu.T)/2 # Approximation trick to make symmetric\n",
    "\n",
    "            # Update log likelihood \n",
    "            loglik = loglik + 0.5*(log(det(iF)) - innov.T.dot(iF).dot(innov))\n",
    "\n",
    "\n",
    "        ### STORE OUTPUT----------------------------------------------------\n",
    "        # Store covariance and observation values for t-1 (priors)\n",
    "        Zm[t, :] = Z\n",
    "        Vm[t, :, :] = V\n",
    "\n",
    "        # Store covariance and state values for t (posteriors)\n",
    "        # i.e. Zu = Z_t|t   & Vu = V_t|t\n",
    "        ZmU[t+1, :] = Zu\n",
    "        VmU[t+1, :, :] = Vu\n",
    "\n",
    "\n",
    "    # Store Kalman gain k_t\n",
    "    if Y_t.size==0:\n",
    "        k_t = np.zeros((m,m))\n",
    "    else:\n",
    "        k_t = VCF.dot(C_t)\n",
    "\n",
    "    return Zm, ZmU, Vm, VmU, loglik, k_t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### runKF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def runKF(Y, A, C, Q, R, Z_0, V_0):\n",
    "    #  Description:\n",
    "    #    runKF() applies a Kalman filter and fixed-interval smoother. The\n",
    "    #    script uses the following model:\n",
    "    #           Y_t = C_t Z_t + e_t for e_t ~ N(0, R)\n",
    "    #           Z_t = A Z_{t-1} + mu_t for mu_t ~ N(0, Q)\n",
    "\n",
    "    #  Throughout this file:\n",
    "    #    'm' denotes the number of elements in the state vector Z_t.\n",
    "    #    'k' denotes the number of elements (observed variables) in Y_t.\n",
    "    #    'nobs' denotes the number of time periods for which data are observed.\n",
    "    #\n",
    "    #  Input parameters:\n",
    "    #    Y: k-by-nobs matrix of input data\n",
    "    #    A: m-by-m transition matrix \n",
    "    #    C: k-by-m observation matrix\n",
    "    #    Q: m-by-m covariance matrix for transition equation residuals (mu_t)\n",
    "    #    R: k-by-k covariance for observation matrix residuals (e_t)\n",
    "    #    Z_0: 1-by-m vector, initial value of state\n",
    "    #    V_0: m-by-m matrix, initial value of state covariance matrix\n",
    "    #\n",
    "    #  Output parameters:\n",
    "    #    zsmooth: k-by-(nobs+1) matrix, smoothed factor estimates\n",
    "    #             (i.e. zsmooth(:,t+1) = Z_t|T)\n",
    "    #    Vsmooth: k-by-k-by-(nobs+1) array, smoothed factor covariance matrices\n",
    "    #             (i.e. Vsmooth(:,:,t+1) = Cov(Z_t|T))\n",
    "    #    VVsmooth: k-by-k-by-nobs array, lag 1 factor covariance matrices\n",
    "    #              (i.e. Cov(Z_t,Z_t-1|T))\n",
    "    #    loglik: scalar, log-likelihood\n",
    "    #\n",
    "    #  References:\n",
    "    #  - QuantEcon's \"A First Look at the Kalman Filter\"\n",
    "    #  - Adapted from replication files for:\n",
    "    #    \"Nowcasting\", 2010, (by Marta Banbura, Domenico Giannone and Lucrezia \n",
    "    #    Reichlin), in Michael P. Clements and David F. Hendry, editors, Oxford \n",
    "    #    Handbook on Economic Forecasting.\n",
    "    #\n",
    "    # The software can be freely used in applications. \n",
    "    # Users are kindly requested to add acknowledgements to published work and \n",
    "    # to cite the above reference in any resulting publications\n",
    "    \n",
    "    states = pd.MultiIndex.from_tuples(A.index)\n",
    "    periods0 = y_est.columns\n",
    "    periods1 = pd.date_range(y_est.columns[0], y_est.columns[-1] + MonthBegin(), freq=y_est.columns.freq)\n",
    "    \n",
    "    periods0_states = pd.MultiIndex.from_tuples([(i, j, p, q) \n",
    "                                                for i in periods0 for j, p, q in states.to_flat_index()])\n",
    "    periods1_states = pd.MultiIndex.from_tuples([(i, j, p, q) \n",
    "                                                  for i in periods1 for j, p, q in states.to_flat_index()])\n",
    "    \n",
    "    Y, A, C, Q, R = Y.to_numpy(), A.to_numpy(), C.to_numpy(), Q.to_numpy(), R.to_numpy()\n",
    "    Z_0, V_0 = Z_0.to_numpy(), V_0.to_numpy()\n",
    "\n",
    "    Zm, ZmU, Vm, VmU, loglik, k_t = SKF(Y, A, C, Q, R, Z_0, V_0)   # Kalman filter\n",
    "    ZmT, VmT, VmT_1 = FIS(A, Zm, ZmU, Vm, VmU, loglik, k_t)        # Fixed interval smoother\n",
    "\n",
    "    # Organize output \n",
    "    Zsmooth = pd.DataFrame(ZmT, index=periods1, columns=states)\n",
    "    \n",
    "    VmT = VmT.reshape(VmT.shape[0]*VmT.shape[1], VmT.shape[2])\n",
    "    VmT_1 = VmT_1.reshape(VmT_1.shape[0]*VmT_1.shape[1], VmT_1.shape[2])\n",
    "    \n",
    "    Vsmooth = pd.DataFrame(VmT, index=periods1_states, columns=states)\n",
    "    VVsmooth = pd.DataFrame(VmT_1, index=periods0_states, columns=states)\n",
    "\n",
    "    return Zsmooth.T, Vsmooth, VVsmooth, loglik, periods0, periods1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EMstep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def EMstep(y, A, C, Q, R, Z_0, V_0, r, p, Rcon, q, blocks, spec):\n",
    "    #  Applies EM algorithm for parameter reestimation\n",
    "    #\n",
    "    #  Syntax:\n",
    "    #  C_new, R_new, A_new, Q_new, Z_0, V_0, loglik\n",
    "    #    = EMstep(y, A, C, Q, R, Z_0, V_0, r, p, Rcon, q, blocks, spec)\n",
    "    #\n",
    "    #  Description:\n",
    "    #    EMstep reestimates parameters based on the Estimation Maximization (EM)\n",
    "    #    algorithm. This is a two-step procedure:\n",
    "    #    (1) E-step: the expectation of the log-likelihood is calculated using\n",
    "    #        previous parameter estimates.\n",
    "    #    (2) M-step: Parameters are re-estimated through the maximisation of\n",
    "    #        the log-likelihood (maximize result from (1)).\n",
    "    #\n",
    "    #    See \"Maximum likelihood estimation of factor models on data sets with\n",
    "    #    arbitrary pattern of missing data\" for details about parameter\n",
    "    #    derivation (Banbura & Modugno, 2010). This procedure is in much the\n",
    "    #    same spirit.\n",
    "    #\n",
    "    #  Input:\n",
    "    #    y:      Series data\n",
    "    #    A:      Transition matrix\n",
    "    #    C:      Observation matrix\n",
    "    #    Q:      Covariance for transition equation residuals\n",
    "    #    R:      Covariance for observation matrix residuals\n",
    "    #    Z_0:    Initial values of factors\n",
    "    #    V_0:    Initial value of factor covariance matrix\n",
    "    #    r:      Number of common factors for each block (e.g. vector [1 1 1 1])\n",
    "    #    p:      Number of lags in transition equation\n",
    "    #    R_mat:  Estimation structure for quarterly variables (i.e. \"tent\")\n",
    "    #    q:      Constraints on loadings\n",
    "    #    nQ:     Number of quarterly series\n",
    "    #    i_idio: Indices for monthly variables\n",
    "    #    blocks: Block structure for each series (i.e. for a series, the structure\n",
    "    #            [1 0 0 1] indicates loadings on the first and fourth factors)\n",
    "    #\n",
    "    #  Output:\n",
    "    #    C_new: Updated observation matrix\n",
    "    #    R_new: Updated covariance matrix for residuals of observation matrix\n",
    "    #    A_new: Updated transition matrix\n",
    "    #    Q_new: Updated covariance matrix for residuals for transition matrix\n",
    "    #    Z_0:   Initial value of state\n",
    "    #    V_0:   Initial value of covariance matrix\n",
    "    #    loglik: Log likelihood\n",
    "    #\n",
    "    # References:\n",
    "    #   \"Maximum likelihood estimation of factor models on data sets with\n",
    "    #   arbitrary pattern of missing data\" by Banbura & Modugno (2010).\n",
    "    #   Abbreviated as BM2010\n",
    "\n",
    "    ## Initialize preliminary values\n",
    "    # Store series/model values\n",
    "    Qv = spec.loc[spec.Frequency.eq('q')].index.tolist()\n",
    "    Mv = spec.loc[spec.Frequency.eq('m')].index.tolist()\n",
    "\n",
    "    n, T = y.shape\n",
    "    nQ = len(Qv)\n",
    "    nM = n - nQ                  # Number of monthly series\n",
    "    pC = Rcon.shape[1]\n",
    "    ppC = max(p,pC)\n",
    "    num_blocks = blocks.shape[1] # Number of blocks\n",
    "\n",
    "    y = y_est.copy()\n",
    "\n",
    "    ## ESTIMATION STEP: Compute the (expected) sufficient statistics for a single\n",
    "    # Kalman filter sequence\n",
    "\n",
    "    # Running the Kalman filter and smoother with current parameters\n",
    "    # Note that log-liklihood is NOT re-estimated after the runKF step: This\n",
    "    # effectively gives the previous iteration's log-likelihood\n",
    "    # For more information on output, see runKF\n",
    "    Zsmooth, Vsmooth, VVsmooth, loglik, p0, p1 = runKF(y, A, C, Q, R, Z_0, V_0)   # df으로 return할 경우 시간이 오래 걸림\n",
    "\n",
    "    ## MAXIMIZATION STEP (TRANSITION EQUATION)\n",
    "    # See (Banbura & Modugno, 2010) for details.\n",
    "\n",
    "    # Initialize output\n",
    "    A_new = A.copy()\n",
    "    Q_new = Q.copy()\n",
    "    V_0_new = V_0.copy()\n",
    "\n",
    "    ### 2A. UPDATE FACTOR PARAMETERS INDIVIDUALLY ----------------------------\n",
    "    for b in blocks.columns:   # Loop for each block: factors are uncorrelated\n",
    "\n",
    "        # ESTIMATE FACTOR PORTION OF Q, A\n",
    "        # Note: EZZ, EZZ_BB, EZZ_FB are parts of equations 6 and 8 in BM 2010\n",
    "\n",
    "        # E[f_t*f_t' | Omega_T]\n",
    "        EZZ = (Zsmooth.loc[b, p1[1:]].dot(Zsmooth.loc[b, p1[1:]].T) \n",
    "               + Vsmooth.loc[idx[p1[1:], b], b].groupby(level=[1, 2, 3], axis=0).sum().loc[b])\n",
    "\n",
    "        # E[f_{t-1}*f_{t-1}' | Omega_T]\n",
    "        EZZ_BB = (Zsmooth.loc[b, p1[:-1]].dot(Zsmooth.loc[b, p1[:-1]].T)\n",
    "                  + Vsmooth.loc[idx[p1[:-1], b], b].groupby(level=[1, 2, 3], axis=0).sum().loc[b])\n",
    "\n",
    "        # E[f_t*f_{t-1}' | Omega_T]\n",
    "        EZZ_FB = (Zsmooth.loc[b, p1[1:]].dot(Zsmooth.shift(axis=1).loc[b, p1[1:]].T)\n",
    "                  + VVsmooth.loc[idx[:, b], b].groupby(level=[1, 2, 3], axis=0).sum().loc[b])\n",
    "\n",
    "        # Select transition matrix/covariance matrix for block i\n",
    "        A_i = A.loc[b, b]\n",
    "        Q_i = Q.loc[b, b]\n",
    "\n",
    "        # Equation 6: Estimate VAR(p) for factor\n",
    "        A_i.loc[0, :p-1] = EZZ_FB.loc[0, :p-1].dot(inv(EZZ_BB.loc[idx[:p-1], idx[:p-1]])).values\n",
    "\n",
    "        # Equation 8: Covariance matrix of residuals of VAR\n",
    "        Q_i.loc[0, 0] = (EZZ.loc[0, 0] - A_i.loc[0, idx[:p-1]].dot(EZZ_FB.loc[0, idx[:p-1]].T)).values / T\n",
    "\n",
    "        # Place updated results in output matrix\n",
    "        A_new.loc[idx[b, :, :], idx[b, :, :]] = A_i.values\n",
    "        Q_new.loc[idx[b, :, :], idx[b, :, :]] = Q_i.values\n",
    "        V_0_new.loc[idx[b, :, :], idx[b, :, :]] = Vsmooth.loc[idx[p1[0], b], b].values\n",
    "\n",
    "    ### 2B. UPDATING PARAMETERS FOR IDIOSYNCRATIC COMPONENT ------------------\n",
    "\n",
    "    # Below 3 estimate the idiosyncratic component (for eqns 6, 8 BM 2010)\n",
    "    # E[f_t*f_t' | \\Omega_T]\n",
    "    EZZ = (diag(diag(Zsmooth.loc[spec.index, p1[1:]].dot(Zsmooth.loc[spec.index, p1[1:]].T))) \n",
    "           + diag(diag(Vsmooth.loc[idx[p1[1:], spec.index], spec.index].groupby(level=[1,2,3],axis=0, sort=False).sum())))\n",
    "\n",
    "    # E[f_{t-1}*f_{t-1}' | \\Omega_T]\n",
    "    EZZ_BB = (diag(diag(Zsmooth.shift(1,axis=1).loc[spec.index, p1[1:]].dot(Zsmooth.shift(1, axis=1).loc[spec.index, p1[1:]].T))) \n",
    "              + diag(diag(Vsmooth.loc[idx[p1[:-1], spec.index], spec.index].groupby(level=[1,2,3],axis=0, sort=False).sum())))\n",
    "\n",
    "    # E[f_t*f_{t-1}' | \\Omega_T]\n",
    "    EZZ_FB = (diag(diag(Zsmooth.loc[spec.index, p1[1:]].dot(Zsmooth.shift(1, axis=1).loc[spec.index, p1[1:]].T))) \n",
    "              + diag(diag(VVsmooth.loc[idx[:, spec.index], spec.index].groupby(level=[1,2,3],axis=0, sort=False).sum())))\n",
    "\n",
    "    states_MQv = Zsmooth.loc[spec.index, p1[1:]].index\n",
    "    A_i = pd.DataFrame(EZZ_FB.dot(diag(1 / diag(EZZ_BB))), index=states_MQv, columns=states_MQv) # Equation 6\n",
    "    Q_i = pd.DataFrame((EZZ - A_i.values.dot(EZZ_FB.T)) / T, index=states_MQv, columns=states_MQv) # Equation 8\n",
    "\n",
    "    # Place updated results in output matrix\n",
    "    A_new.loc[idx[Mv, :, :], idx[Mv, :, :]] = A_i.loc[Mv, Mv].values\n",
    "    Q_new.loc[idx[Mv, :, :], idx[Mv, :, :]] = Q_i.loc[Mv, Mv].values\n",
    "    V_0_new.loc[idx[Mv, :, :], idx[Mv, :, :]] = diag(diag(Vsmooth.loc[idx[p1[0], Mv], Mv]))\n",
    "\n",
    "    ## 3 MAXIMIZATION STEP (observation equation)\n",
    "\n",
    "    ### INITIALIZATION AND SETUP ----------------------------------------------\n",
    "    Z_0 = Zsmooth.loc[:, p1[0]] # zeros(size(Zsmooth,1),1); #\n",
    "\n",
    "    # Set missing data series values to 0\n",
    "    nanY = y.isnull()\n",
    "    y[nanY] = 0\n",
    "\n",
    "    # LOADINGS\n",
    "    C_new = C.copy()\n",
    "\n",
    "    R_con = empty((0, 0), int)\n",
    "    q_con = empty((0, 1), int)\n",
    "\n",
    "    for r_i in r:\n",
    "        R_con = block_diag(R_con, kron(Rcon, eye(r_i)))\n",
    "        q_con = np.append(q_con, np.zeros((r_i*Rcon.shape[0], 1)), axis=0)\n",
    "\n",
    "    R_con = pd.DataFrame(R_con, columns = A.loc[blocks.columns, blocks.columns].columns)\n",
    "\n",
    "    blocks_sum = blocks.copy()\n",
    "    blocks_sum['bc'] = blocks.dot([1, 2, 3, 4])\n",
    "    blocks_sum = blocks_sum.set_index('bc', append=True).reorder_levels([1, 0],axis=0).sort_index(axis=0)\n",
    "    bl = blocks_sum.drop_duplicates().loc[[1, 5, 4, 3]]\n",
    "\n",
    "    for i, row in bl.eq(1).iterrows():\n",
    "\n",
    "        bs = blocks_sum.drop_duplicates().columns[row].tolist()\n",
    "        rs = sum(r[bs])\n",
    "\n",
    "        idx_iM = [v for v in Mv if v in blocks_sum.loc[i[0]].index.tolist()]  # variables need to be ordered as in Mv!\n",
    "        n_i = len(idx_iM)  \n",
    "\n",
    "        # Initialize sums in equation 13 of BGR 2010\n",
    "        denom = zeros((n_i*rs,n_i*rs))    \n",
    "        nom = zeros((n_i,rs))   \n",
    "\n",
    "        ### UPDATE MONTHLY VARIABLES: Loop through each period ----------------\n",
    "        Wd = ~nanY.loc[idx_iM, :].to_numpy()\n",
    "        Zd = Zsmooth.loc[idx[bs, 0, :], :].to_numpy()\n",
    "        Vd = Vsmooth.loc[idx[:, bs, 0, :], idx[bs, 0, :]].to_numpy().reshape(T+1, rs, rs)\n",
    "        yd = y.loc[idx_iM, :].to_numpy()\n",
    "        Ze = Zsmooth.loc[idx[idx_iM, 0, :], :].to_numpy()\n",
    "        Ve = Vsmooth.loc[idx[:, idx_iM, 0, :], idx[bs, 0, :]].to_numpy().reshape(T+1, n_i, rs)\n",
    "\n",
    "        for t in range(y.shape[1]):\n",
    "            Wt = diag(Wd[:, t])  # Gives selection matrix (1 for nonmissing values)\n",
    "\n",
    "            # E[f_t*t_t' | Omega_T]\n",
    "            denom = (denom + kron(Zd[:, [t+1]].dot(Zd[:, [t+1]].T) + Vd[t+1, :, :], Wt))\n",
    "\n",
    "            # E[y_t*f_t' | \\Omega_T]\n",
    "            nom = (nom + yd[:, [t]].dot(Zd[:, [t+1]].T) \n",
    "                   - Wt[:, :].dot(Ze[:, [t+1]].dot(Zd[:, [t+1]].T) + Ve[t+1, :, :]))\n",
    "\n",
    "        vec_C = inv(denom).dot(nom.flatten('F'))  # Eqn 13 BGR 2010\n",
    "\n",
    "        # Place updated monthly results in output matrix\n",
    "        C_new.loc[idx_iM, idx[bs, 0, :]] = vec_C.reshape(n_i, rs, order='F')\n",
    "\n",
    "\n",
    "       ### UPDATE QUARTERLY VARIABLES -----------------------------------------\n",
    "        idx_iQ = [v for v in Qv if v in blocks_sum.loc[i[0]].index.tolist()]  # Index for quarterly series\n",
    "\n",
    "        # Monthly-quarterly aggregation scheme\n",
    "        R_con_i = R_con.loc[:, bs].to_numpy()\n",
    "        q_con_i = q_con.copy()\n",
    "\n",
    "        # select non-zero rows of R_con_i, q_con_i\n",
    "        sel_rows = np.any(R_con_i, axis=1)\n",
    "        R_con_i = R_con_i[sel_rows, :]\n",
    "        q_con_i = q_con_i[sel_rows, :]\n",
    "\n",
    "        # Loop through quarterly series in loading. This parallels monthly code\n",
    "        for j in idx_iQ:\n",
    "\n",
    "            # Initialization\n",
    "            denom = zeros((rs*ppC, rs*ppC))\n",
    "            nom = zeros((1,rs*ppC))\n",
    "\n",
    "            # Place quarterly values in output matrix\n",
    "            V_0_new.loc[idx[j, :, :], idx[j, :, :]] = Vsmooth.loc[p0[0]].loc[idx[j, :, :], idx[j, :, :]]\n",
    "            A_new.loc[idx[j, 0, 1], idx[j, 0, 1]] = A_i.loc[idx[j, 0, 1], idx[j, 0, 1]]\n",
    "            Q_new.loc[idx[j, 0, 1], idx[j, 0, 1]] = Q_i.loc[idx[j, 0, 1], idx[j, 0, 1]]\n",
    "\n",
    "\n",
    "            Wd = ~nanY.loc[[j], :].to_numpy()\n",
    "            Zd = Zsmooth.loc[idx[bs, :, :], :].to_numpy()\n",
    "            Vd = Vsmooth.loc[idx[:, bs, :, :], idx[bs, :, :]].to_numpy().reshape(T+1, rs*ppC, rs*ppC, order='F')\n",
    "            yd = y.loc[[j], :].to_numpy()\n",
    "            Ze = Zsmooth.loc[idx[j, :, :], :].to_numpy()\n",
    "            Ve = Vsmooth.loc[idx[:, j, :, :], idx[bs, :, :]].to_numpy().reshape(T+1, ppC, rs*ppC, order='F')\n",
    "\n",
    "            for t in range(y.shape[1]):\n",
    "                Wt = diag(Wd[:, t])  # Selection matrix for quarterly values\n",
    "\n",
    "                # Intermediate steps in BGR equation 13\n",
    "                denom = (denom + kron(Zd[:, [t+1]].dot(Zd[:, [t+1]].T) + Vd[t+1, :, :], Wt))\n",
    "                nom = nom + yd[:, [t]].dot(Zd[:, [t+1]].T)\n",
    "                nom = (nom - Wt.dot(np.array([[1, 2, 3, 2, 1]]).dot(Ze[:, [t+1]]).dot(Zd[:, [t+1]].T) \n",
    "                                    + np.array([[1, 2, 3, 2, 1]]).dot(Ve[t+1, :, :])))\n",
    "\n",
    "            C_i = inv(denom).dot(nom.T)\n",
    "\n",
    "            # BGR equation 13\n",
    "            inv_RdR = inv(R_con_i.dot(inv(denom)).dot(R_con_i.T))\n",
    "            C_i_constr = (C_i - inv(denom).dot(R_con_i.T).dot(inv_RdR).dot(R_con_i.dot(C_i)-q_con_i))\n",
    "\n",
    "            # Place updated values in output structure\n",
    "            C_new.loc[[j], idx[bs, :, :]] = C_i_constr.T\n",
    "\n",
    "    ### 3B. UPDATE COVARIANCE OF RESIDUALS FOR OBSERVATION EQUATION -----------\n",
    "    # Initialize covariance of residuals of observation equation\n",
    "    R_new = zeros((n,n))\n",
    "\n",
    "    Wd = ~nanY.to_numpy()\n",
    "    Zd = Zsmooth.to_numpy()\n",
    "    Vd = Vsmooth.to_numpy().reshape(T+1, A.shape[0], A.shape[1])\n",
    "    yd = y.loc[:, :].to_numpy()\n",
    "    Ze = Zsmooth.loc[idx[j, :, :], :].to_numpy()\n",
    "    Ve = Vsmooth.loc[idx[:, j, :, :], idx[bs, :, :]].to_numpy().reshape(T+1, ppC, rs*ppC)\n",
    "\n",
    "    for t in range(y.shape[1]):\n",
    "        Wt = diag(Wd[:, t])            # Selection matrix\n",
    "        # BGR equation 15\n",
    "        R_new = (R_new + (yd[:, [t]] - Wt.dot(C_new).dot(Zd[:, [t+1]])).dot((yd[:, [t]] - Wt.dot(C_new).dot(Zd[:, [t+1]])).T)\n",
    "                 + Wt.dot(C_new).dot(Vd[t+1, :, :]).dot(C_new.T).dot(Wt) + (eye(n)-Wt).dot(R).dot(eye(n) - Wt))\n",
    "\n",
    "    R_new = R_new / T\n",
    "    R_new = pd.DataFrame(diag(diag(R_new)), index=R.index, columns=R.columns) #RR(RR<1e-2) = 1e-2;\n",
    "\n",
    "    for v in Mv:\n",
    "        R_new.loc[v, v] = 1e-04\n",
    "\n",
    "    for v in Qv:\n",
    "        R_new.loc[v, v] = 1e-04\n",
    "\n",
    "    return C_new, R_new, A_new, Q_new, Z_0, V_0, loglik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### em_converged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def em_converged(loglik, previous_loglik, threshold=1e-4, check_decreased=1):\n",
    "    #  checks whether EM algorithm has converged\n",
    "    #\n",
    "    #  Syntax:\n",
    "    #    [converged, decrease] = em_converged(loglik, previous_loglik, threshold, check_increased)\n",
    "    #\n",
    "    #  Description:\n",
    "    #    em_converged() checks whether EM has converged. Convergence occurs if\n",
    "    #    the slope of the log-likelihood function falls below 'threshold'(i.e.\n",
    "    #    f(t) - f(t-1)| / avg < threshold) where avg = (|f(t)| + |f(t-1)|)/2\n",
    "    #    and f(t) is log lik at iteration t. 'threshold' defaults to 1e-4.\n",
    "    #\n",
    "    #    This stopping criterion is from Numerical Recipes in C (pg. 423).\n",
    "    #    With MAP estimation (using priors), the likelihood can decrease\n",
    "    #    even if the mode of the posterior increases.\n",
    "    #\n",
    "    #  Input arguments:\n",
    "    #    loglik: Log-likelihood from current EM iteration\n",
    "    #    previous_loglik: Log-likelihood from previous EM iteration\n",
    "    #    threshold: Convergence threshhold. The default is 1e-4.\n",
    "    #    check_decreased: Returns text output if log-likelihood decreases.\n",
    "    #\n",
    "    #  Output:\n",
    "    #    converged (numeric): Returns 1 if convergence criteria satisfied, and 0 otherwise.\n",
    "    #    decrease (numeric): Returns 1 if loglikelihood decreased.\n",
    "\n",
    "    ## Instantiate variables\n",
    "    # Threshhold arguments: Checks default behavior\n",
    "    # if nargin < 3, threshold = 1e-4; end\n",
    "    # if nargin < 4, check_decreased = 1; end\n",
    "\n",
    "    # Initialize output\n",
    "    converged = 0;\n",
    "    decrease = 0;\n",
    "\n",
    "    ## Check if log-likelihood decreases (optional)\n",
    "    if check_decreased:\n",
    "        if loglik < previous_loglik - 1e-3: # allow for a little imprecision\n",
    "            print('loglik decreased from {:,.4f} to {:,.4f}'.format(previous_loglik, loglik))\n",
    "            decrease = 1\n",
    "\n",
    "    ## Check convergence criteria\n",
    "    delta_loglik = abs(loglik - previous_loglik)               # Difference in loglik\n",
    "    avg_loglik = (abs(loglik) + abs(previous_loglik) + np.spacing(1))/2\n",
    "\n",
    "    if (delta_loglik / avg_loglik) < threshold:\n",
    "        converged = 1                                          # Check convergence\n",
    "\n",
    "    return converged, decrease, (delta_loglik / avg_loglik)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `Spec`, `Data`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User inputs.\n",
    "vintage = '2016-06-29'; # vintage dataset to use for estimation\n",
    "country = 'US';         # United States macroeconomic data\n",
    "sample_start  = '2000-01-01'; # estimation sample\n",
    "\n",
    "Spec = load_spec('Spec_US_example.xls');\n",
    "# Parse `Spec`\n",
    "# SeriesID = Spec.SeriesID; SeriesName = Spec.SeriesName; Units = Spec.Units; UnitsTransformed = Spec.UnitsTransformed;\n",
    "\n",
    "datafile = path.join('data', country, vintage + '.xls')\n",
    "X, Z = load_data(datafile, Spec, sample_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, axs = plt.subplots(1, 2, figsize=(16, 4))\n",
    "\n",
    "# X.INDPRO.plot(ax=axs[0])\n",
    "# Z.INDPRO.plot(ax=axs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T, N = X.shape;\n",
    "# nQ = sum(Spec.Spec.Frequency.eq('q'));   # Number of quarterly series\n",
    "# nM = N - nQ                              # number of monthly series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "blocks = Spec.Blocks;                    # Block loading structure\n",
    "spec = Spec.Spec;                        # Block loading structure\n",
    "p = 1;                                   # Number of lags in autoregressive of factor\n",
    "r = pd.Series(np.ones(blocks.shape[1], dtype=int), \n",
    "              index=blocks.columns)  # Number of common factors for each block\n",
    "# i_idio = np.array([True] * nM + [False] * nQ);\n",
    "\n",
    "# R*Lambda = q; Contraints on the loadings of the quartrly variables\n",
    "Rcon = np.array([[2, -1,  0,  0,  0],\n",
    "                 [3,  0, -1,  0,  0],\n",
    "                 [2,  0,  0, -1,  0],\n",
    "                 [1,  0,  0,  0, -1]])\n",
    "q = np.zeros(4);\n",
    "\n",
    "## DATA Normalization and InitCond\n",
    "Mx = X.mean(skipna=True)\n",
    "Wx = X.std(skipna=True)\n",
    "xNaN = (X - Mx)/Wx        # Standardize series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InitCond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optNaN = pd.Series({'method':2, 'k':3})\n",
    "# method=2 : Remove leading and closing zeros\n",
    "# k=3      : Setting for filter(): See remNaN_spline\n",
    "\n",
    "A, C, Q, R, Z_0, V_0 = InitCond(xNaN, r, p, spec, blocks, optNaN, Rcon, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EM LOOP\n",
    "\n",
    "$y_t = CZ_t + e \\newline\n",
    "Z_t = AZ_{t-1} + v, \\newline\n",
    "\\text{where} y: N \\times T, Z: pr \\times T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loglik decreased from -1,249.1730 to -1,249.5072\n",
      "loglik decreased from -1,249.5072 to -1,250.5757\n",
      "loglik decreased from -1,250.5757 to -1,252.4924\n",
      "loglik decreased from -1,252.4924 to -1,255.4063\n",
      "loglik decreased from -1,255.4063 to -1,259.4991\n",
      "loglik decreased from -1,259.4991 to -1,264.9831\n",
      "loglik decreased from -1,264.9831 to -1,272.0940\n",
      "loglik decreased from -1,272.0940 to -1,281.0719\n",
      "loglik decreased from -1,281.0719 to -1,292.1193\n",
      "loglik decreased from -1,292.1193 to -1,305.3813\n",
      "loglik decreased from -1,305.3813 to -1,321.2335\n",
      "loglik decreased from -1,321.2335 to -1,341.1551\n",
      "loglik decreased from -1,341.1551 to -1,367.3865\n",
      "loglik decreased from -1,367.3865 to -1,400.8836\n",
      "loglik decreased from -1,400.8836 to -1,440.9451\n",
      "loglik decreased from -1,440.9451 to -1,485.7137\n",
      "loglik decreased from -1,485.7137 to -1,532.4111\n",
      "loglik decreased from -1,532.4111 to -1,577.1465\n",
      "loglik decreased from -1,577.1465 to -1,618.6355\n",
      "loglik decreased from -1,618.6355 to -1,658.6754\n",
      "loglik decreased from -1,658.6754 to -1,674.1772\n",
      "loglik decreased from -1,619.5078 to -1,646.0061\n",
      "loglik decreased from -1,621.0942 to -1,622.6103\n"
     ]
    }
   ],
   "source": [
    "threshold = 1e-4;  # EM loop threshold (default value)\n",
    "max_iter = 5000;   # EM loop maximum number of iterations\n",
    "\n",
    "# Initialize EM loop values\n",
    "previous_loglik = -np.inf\n",
    "num_iter = 0\n",
    "LL = np.array([-np.inf])\n",
    "converged = 0\n",
    "\n",
    "# y for the estimation is WITH missing data\n",
    "y = xNaN.T\n",
    "\n",
    "# Remove the leading and ending nans\n",
    "optNaN.method = 3\n",
    "y_est = remNaNs_spline(xNaN, optNaN)[0].T\n",
    "\n",
    "for num_iter in range(max_iter):    # Loop until converges or max iter.\n",
    "    \n",
    "    if converged:\n",
    "        break\n",
    "\n",
    "    # Applying EM algorithm\n",
    "    C_new, R_new, A_new, Q_new, Z_0, V_0, loglik = EMstep(y_est, A, C, Q, R, Z_0, V_0, r, p, Rcon, q, blocks, spec)\n",
    "\n",
    "    C = C_new\n",
    "    R = R_new\n",
    "    A = A_new\n",
    "    Q = Q_new\n",
    "\n",
    "    if num_iter > 2:  # Checking convergence\n",
    "        converged, decrease, diff_ratio = em_converged(loglik, previous_loglik, threshold, 1)\n",
    "\n",
    "    if (np.mod(num_iter, 50) == 0) & (num_iter > 0):  # Print updates to command window\n",
    "        print('Now running the {:}'.format(num_iter), 'th iteration of max {:}'.format(max_iter))\n",
    "        print('Loglik: {:.4},  {:,.2f} (% Change), diff_ratio: {:,.4}'.format(loglik, \n",
    "                                                                              100*(loglik/previous_loglik-1),\n",
    "                                                                              100*diff_ratio))\n",
    "    LL = np.append(LL, 4)\n",
    "    previous_loglik = loglik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "Estimating the dynamic factor model (DFM) ... \n",
    "\n",
    "Now running the 10th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1244.3883   ( -0.10%)\n",
    "Now running the 20th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1237.5744   ( -0.04%)\n",
    "Now running the 30th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1234.4234   ( -0.02%)\n",
    "Now running the 40th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1232.3937   ( -0.02%)\n",
    "Now running the 50th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1230.1691   ( -0.02%)\n",
    "Now running the 60th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1227.316   ( -0.02%)\n",
    "Now running the 70th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1225.2472   ( -0.02%)\n",
    "Now running the 80th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1223.3936   ( -0.01%)\n",
    "Now running the 90th iteration of max 5000\n",
    "  Loglik   (% Change)\n",
    "-1221.9667   ( -0.01%)\n",
    "Successful: Convergence at 92 iterations\n",
    "\n",
    "\n",
    "\n",
    "Table 4: Factor Loadings for Monthly Series\n",
    "                                    Global       Soft        Real        Labor  \n",
    "                                  __________    _______    _________    ________\n",
    "\n",
    "    Payroll_Employment                0.2518          0            0     0.59252\n",
    "    Job_Openings                    0.094528          0            0     0.42164\n",
    "    Consumer_Price_Index             0.24999          0            0           0\n",
    "    Durable_Goods_Orders             0.12301          0     0.020417           0\n",
    "    Retail_Sales                     0.21756          0      0.50378           0\n",
    "    Unemployment_Rate                 -0.187          0            0    -0.68573\n",
    "    Housing_Starts                   0.10429          0      0.28356           0\n",
    "    Industrial_Production            0.24871          0     -0.15552           0\n",
    "    Personal_Income               -0.0056609          0      0.15363           0\n",
    "    Exports                          0.27828          0    -0.030817           0\n",
    "    Imports                          0.27296          0    -0.061193           0\n",
    "    Construction_Spending            0.12748          0      0.29406           0\n",
    "    Import_Price_Index               0.28256          0            0           0\n",
    "    Core_Consumer_Price_Index       0.037066          0            0           0\n",
    "    Core_PCE_Price_Index             0.12616          0            0           0\n",
    "    PCE_Price_Index                  0.27298          0            0           0\n",
    "    Building_Permits                0.095284          0      0.39561           0\n",
    "    Capacity_Utilization_Rate        0.24671          0     -0.16996           0\n",
    "    Business_Inventories             0.26078          0     -0.25369           0\n",
    "    Export_Price_Index               0.26084          0            0           0\n",
    "    Empire_State_Mfg_Index           0.24974    0.66818            0           0\n",
    "    Real_Consumption_Spending        0.13534          0      0.52571           0\n",
    "    Philadelphia_Fed_Mfg_Index       0.26133     0.7442            0           0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 5: Quarterly Loadings Sample (Global Factor)\n",
    "                                   f1_lag0     f1_lag1     f1_lag2     f1_lag3     f1_lag4 \n",
    "                                   ________    ________    ________    ________    ________\n",
    "\n",
    "    Real_Gross_Domestic_Product    0.035198    0.070396     0.10559    0.070396    0.035198\n",
    "    Unit_Labor_Cost                0.006151    0.012302    0.018453    0.012302    0.006151\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 6: Autoregressive Coefficients on Factors\n",
    "              AR_Coefficient    Variance_Residual\n",
    "              ______________    _________________\n",
    "\n",
    "    Global        0.84172             1.4054     \n",
    "    Soft          0.86641            0.19808     \n",
    "    Real        -0.041791             1.1363     \n",
    "    Labor         0.95751           0.039662     \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Table 7: Autoregressive Coefficients on Idiosyncratic Component\n",
    "                                   AR_Coefficient    Variance_Residual\n",
    "                                   ______________    _________________\n",
    "\n",
    "    Payroll_Employment                 0.49041            0.25514     \n",
    "    Job_Openings                      -0.42855            0.78129     \n",
    "    Consumer_Price_Index               0.33285            0.58469     \n",
    "    Durable_Goods_Orders              -0.43084            0.74744     \n",
    "    Retail_Sales                      -0.34587            0.33667     \n",
    "    Unemployment_Rate                 -0.16861            0.60519     \n",
    "    Housing_Starts                    -0.44569            0.72348     \n",
    "    Industrial_Production            -0.064474            0.64904     \n",
    "    Personal_Income                   -0.19679            0.93442     \n",
    "    Exports                           -0.32068             0.5141     \n",
    "    Imports                           -0.43439            0.47465     \n",
    "    Construction_Spending              0.35339            0.75665     \n",
    "    Import_Price_Index                 0.45995            0.39077     \n",
    "    Core_Consumer_Price_Index          0.22301            0.92217     \n",
    "    Core_PCE_Price_Index              -0.12474            0.91553     \n",
    "    PCE_Price_Index                     0.2869             0.5562     \n",
    "    Building_Permits                  -0.31446            0.79755     \n",
    "    Capacity_Utilization_Rate        -0.058891            0.63438     \n",
    "    Business_Inventories               0.63575            0.33717     \n",
    "    Export_Price_Index                 0.27001            0.53792     \n",
    "    Empire_State_Mfg_Index             0.53826            0.22165     \n",
    "    Real_Consumption_Spending         -0.34755            0.45283     \n",
    "    Philadelphia_Fed_Mfg_Index        0.042402            0.13256     \n",
    "    Real_Gross_Domestic_Product       -0.46611           0.045279     \n",
    "    Unit_Labor_Cost                   -0.88883           0.087425     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
